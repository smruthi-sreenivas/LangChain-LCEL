{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smruthi-sreenivas/LangChain-LCEL/blob/main/Langchain_LCEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LCEL (LangChain Expression Language) simplifies syntax and it's easy to use for simple chains, but it can get confusing once we add complexity."
      ],
      "metadata": {
        "id": "EgLvqhMgJ7nK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SQW6tyx_J1zR",
        "outputId": "8c68630a-a809-4a68-bbf7-75dd842df351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.15.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.67)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.33.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.2)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.14.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Downloading cohere-5.15.0-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: types-requests, python-dotenv, mypy-extensions, marshmallow, httpx-sse, fastavro, typing-inspect, pydantic-settings, dataclasses-json, cohere, langchain-community\n",
            "Successfully installed cohere-5.15.0 dataclasses-json-0.6.7 fastavro-1.11.1 httpx-sse-0.4.0 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 types-requests-2.32.4.20250611 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain cohere langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_community.llms import Cohere\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "Uvd_gYsQKfyB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"{question}\")\n",
        "combine_answers_prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"Given the question: {question} and the below two answers:\n",
        "Answer 1:\n",
        "{answer1}\n",
        "\n",
        "Answer 2:\n",
        "{answer2}\n",
        "\n",
        "Combine the viewpoints of two answers and form a coherent combined answer.\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "gnisCuMWKFJq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Cohere(cohere_api_key=userdata.get('COHERE_KEY'),temperature=0.2)\n",
        "model2 = Cohere(cohere_api_key=userdata.get('COHERE_KEY'),temperature=0.2)\n",
        "model3 = Cohere(cohere_api_key=userdata.get('COHERE_KEY'),temperature=0.2)\n",
        "\n",
        "chain1 = prompt | model1 | StrOutputParser()\n",
        "chain2 = prompt | model2 | StrOutputParser()\n",
        "chain3 = combine_answers_prompt | model3 | StrOutputParser()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szXcKXHUKkZB",
        "outputId": "bb984afd-f94f-43e2-b2de-1b0ad5c57048"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-713428999.py:1: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
            "  model1 = Cohere(cohere_api_key=userdata.get('COHERE_KEY'),temperature=0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What's the best way to stay up to date with latest Large Language Model news? Please keep the answer short and concise, limit to 3 bullet points.\""
      ],
      "metadata": {
        "id": "nqo5m1BlKsyY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer1_output = chain1.invoke(question)"
      ],
      "metadata": {
        "id": "nJe5zfEJK0xL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer1_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2xPzYSfLCmv",
        "outputId": "5d82419c-fb91-4166-9861-86ebd59b325c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are three ways to stay informed about the latest news and updates regarding Large Language Models: \n",
            "\n",
            "1. Follow Social Media Accounts: Many researchers, developers, and organizations involved in LLM research and innovation regularly post updates on social media platforms like Twitter. By following key accounts and influencers, you can get a stream of the latest news, articles, and insights about LLMs. \n",
            "\n",
            "2. Subscribe to Research Blogs: Several academic institutions and technology companies working in the field of AI and LLMs maintain research blogs. These blogs provide detailed updates on their work, advancements, and discoveries. Subscribing to these blogs ensures you receive updates via email or RSS feeds as soon threads are published. \n",
            "\n",
            "3. AI and ML Online Publications: Dedicated online publications and magazines specifically focused on AI and Machine Learning offer in-depth coverage of LLM news and developments. These outlets provide comprehensive articles, interviews, and analysis about various aspects of LLMs, including advancements in technology, applications, and implications. \n",
            "\n",
            "By combining these three methods, you can create a comprehensive information stream that will keep you informed about the latest Large Language Model news and updates. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer2_output = chain2.invoke(question)"
      ],
      "metadata": {
        "id": "sPOp2XVGK1fA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer2_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn1Rk6yLLDPM",
        "outputId": "48db3adb-469d-4621-cccc-f060f79753b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are three ways to stay informed about the latest news and updates regarding Large Language Models: \n",
            "\n",
            "1. Follow Social Media Accounts: Many researchers, developers, and organizations involved in LLM research and innovation regularly share updates on social media platforms like Twitter. By following key accounts and influencers, you can stay informed about the latest advancements, papers, and discussions related to LLMs. \n",
            "\n",
            "2. Subscribe to Research Blogs: Subscribe to influential blogs and websites that offer in-depth coverage of LLM news and developments. These platforms often cover a range of topics, including the latest research papers, model performances, and insights from experts in the field. \n",
            "\n",
            "3. Attend Conferences and Events: Attending conferences and events related to AI and natural language processing can provide valuable opportunities to stay informed about the latest LLM innovations and network with experts in the field. These gatherings often feature keynote speeches, panel discussions, and workshops that delve into the advancements and future prospects of large language models. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_answer_output = chain3.invoke({\"question\": question, \"answer1\": answer1_output, \"answer2\": answer2_output})"
      ],
      "metadata": {
        "id": "Io0l0Dm9K2lg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_answer_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Otu5bsqLJP4",
        "outputId": "7f2dcd97-a9fc-46ee-ce85-72087c3405d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are three ways to stay informed about the latest news and updates regarding Large Language Models: \n",
            "\n",
            "1. Follow Social Media Accounts: Many researchers, developers, and organizations involved in LLM research and innovation regularly share updates on social media platforms like Twitter. By following key accounts and influencers, you can stay informed about the latest advancements, papers, and discussions related to LLMs. \n",
            "\n",
            "2. Subscribe to Research Blogs: Subscribe to influential blogs and websites that offer in-depth coverage of LLM news and developments. These platforms often cover a range of topics, including the latest research papers, model performances, and insights from experts in the field. \n",
            "\n",
            "3. Attend Conferences and Events: Attending conferences and events related to AI and natural language processing can provide valuable opportunities to stay informed about the latest LLM innovations and network with experts in the field. These gatherings often feature keynote speeches, panel discussions, and workshops that delve into the advancements and future prospects of large language models. \n",
            "\n",
            "By combining these three methods, you can create a comprehensive information stream that will keep you informed about the latest Large Language Model news and updates. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"answer1\": chain1,\n",
        "    \"answer2\": chain2,\n",
        "} | chain3\n",
        "\n",
        "combined_answer = combined_chain.invoke(question)"
      ],
      "metadata": {
        "id": "AgRYc8yKK9o6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfqEHoZXLUjB",
        "outputId": "007f9f9c-1a42-46d1-845c-19d1a5029b8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are three ways to stay informed about the latest news and updates regarding Large Language Models: \n",
            "\n",
            "- Follow research organizations and institutions: Academic institutions and companies actively working on LLM research and development often have publications, blogs, and news sections that provide insights into their latest advancements and announcements. \n",
            "- Subscribe to AI newsletters: Several newsletters specialize in Artificial Intelligence and Large Language Model news, providing concise summaries and links to significant developments and articles in the field. \n",
            "- Use social media and online communities: Platforms like Twitter, Reddit, and GitHub have active forums and threads dedicated to LLM discussions. Engaging with these communities provides real-time updates and diverse perspectives on the latest developments. \n",
            "\n",
            "By leveraging these options, you can stay informed about the latest advancements and news related to Large Language Models. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_chain = (\n",
        "    {\"question\": RunnablePassthrough()}\n",
        "    | RunnablePassthrough.assign(answer1=chain1)\n",
        "    | RunnablePassthrough.assign(answer2=chain2)\n",
        "    | chain3\n",
        ")"
      ],
      "metadata": {
        "id": "YbEWm2qcK-46"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X3jKd0KLWIo",
        "outputId": "9271ca69-8736-4cbc-d46a-71f72d29923e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first={\n",
            "  question: RunnablePassthrough()\n",
            "} middle=[RunnableAssign(mapper={\n",
            "  answer1: ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
            "           | Cohere(client=<cohere.client.Client object at 0x7b43ce845290>, async_client=<cohere.client.AsyncClient object at 0x7b43cdbb11d0>, temperature=0.2, cohere_api_key=SecretStr('**********'))\n",
            "           | StrOutputParser()\n",
            "}), RunnableAssign(mapper={\n",
            "  answer2: ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
            "           | Cohere(client=<cohere.client.Client object at 0x7b43cdb3a850>, async_client=<cohere.client.AsyncClient object at 0x7b43cd46c410>, temperature=0.2, cohere_api_key=SecretStr('**********'))\n",
            "           | StrOutputParser()\n",
            "}), ChatPromptTemplate(input_variables=['answer1', 'answer2', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['answer1', 'answer2', 'question'], input_types={}, partial_variables={}, template='Given the question: {question} and the below two answers:\\nAnswer 1:\\n{answer1}\\n\\nAnswer 2:\\n{answer2}\\n\\nCombine the viewpoints of two answers and form a coherent combined answer.\\n'), additional_kwargs={})]), Cohere(client=<cohere.client.Client object at 0x7b43cd46db50>, async_client=<cohere.client.AsyncClient object at 0x7b43cd46e410>, temperature=0.2, cohere_api_key=SecretStr('**********'))] last=StrOutputParser()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h21zALp9vYJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}